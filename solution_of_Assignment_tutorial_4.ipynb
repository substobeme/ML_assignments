{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/substobeme/ML_assignments/blob/main/solution_of_Assignment_tutorial_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tutorial 4 Assignment - Logistic Regression\n",
        "\n",
        "We have provided you with a preprocessed dataset, the first cell will load and set everything up for you.\n",
        "The objectives for you to complete are as follows:\n",
        "1. Code up the commented functions on your own.\n",
        "2. Every step that you must code are explained as comments, use them as hints.\n",
        "\n",
        "The last cell has the code set up for training the model. We expect each one to have trained the model, and note down the best accuracy that they can achieve, and the conditions required to do the same."
      ],
      "metadata": {
        "id": "1k2vhsMVv0Pk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O dataset.csv \"https://docs.google.com/spreadsheets/d/1RNtDIvisrnOmjJxS7aPm-45NtOH3qd5-mgd2bHeSOGA/export?format=csv&gid=1727131321\"\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "df=pd.read_csv('/content/dataset.csv')\n",
        "\n",
        "df.head()\n",
        "X = df.drop(['RainTomorrow'], axis=1)\n",
        "y = df['RainTomorrow']\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42, stratify=y)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nj3rHkttqucf",
        "outputId": "ac3298cf-c0ac-4d50-dad7-876d8f4ec151"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-09-02 09:24:03--  https://docs.google.com/spreadsheets/d/1RNtDIvisrnOmjJxS7aPm-45NtOH3qd5-mgd2bHeSOGA/export?format=csv&gid=1727131321\n",
            "Resolving docs.google.com (docs.google.com)... 74.125.26.113, 74.125.26.138, 74.125.26.100, ...\n",
            "Connecting to docs.google.com (docs.google.com)|74.125.26.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 307 Temporary Redirect\n",
            "Location: https://doc-00-c8-sheets.googleusercontent.com/export/54bogvaave6cua4cdnls17ksc4/pnm28dfpmpoal8384emna33sss/1725269040000/112261653790527273724/*/1RNtDIvisrnOmjJxS7aPm-45NtOH3qd5-mgd2bHeSOGA?format=csv&gid=1727131321 [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2024-09-02 09:24:03--  https://doc-00-c8-sheets.googleusercontent.com/export/54bogvaave6cua4cdnls17ksc4/pnm28dfpmpoal8384emna33sss/1725269040000/112261653790527273724/*/1RNtDIvisrnOmjJxS7aPm-45NtOH3qd5-mgd2bHeSOGA?format=csv&gid=1727131321\n",
            "Resolving doc-00-c8-sheets.googleusercontent.com (doc-00-c8-sheets.googleusercontent.com)... 173.194.210.132, 2607:f8b0:400c:c0f::84\n",
            "Connecting to doc-00-c8-sheets.googleusercontent.com (doc-00-c8-sheets.googleusercontent.com)|173.194.210.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/csv]\n",
            "Saving to: ‘dataset.csv’\n",
            "\n",
            "dataset.csv             [           <=>      ]  12.04M  5.48MB/s    in 2.2s    \n",
            "\n",
            "2024-09-02 09:24:10 (5.48 MB/s) - ‘dataset.csv’ saved [12621972]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CODE BELOW"
      ],
      "metadata": {
        "id": "kiZ4TBA6wFea"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2A-RJ6Rscyoa"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def sigmoid(z):\n",
        "    \"\"\"\n",
        "    Compute the sigmoid function.\n",
        "\n",
        "    Parameters:\n",
        "    z : numpy array\n",
        "        Linear combination of weights and input features.\n",
        "\n",
        "    Returns:\n",
        "    numpy array\n",
        "        Sigmoid of input z.\n",
        "    \"\"\"\n",
        "    z = np.clip(z, -709, 709)\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "\n",
        "def initialize_weights(n_features):\n",
        "    \"\"\"\n",
        "    Initialize weights and bias to zero.\n",
        "\n",
        "    Parameters:\n",
        "    n_features : int\n",
        "        Number of features in the dataset.\n",
        "\n",
        "    Returns:\n",
        "    tuple\n",
        "        Initialized weights and bias.\n",
        "    \"\"\"\n",
        "    # initialize the weights and bias to zero (hint: make sure dimentions are correct)\n",
        "    weights = np.zeros(n_features)\n",
        "    bias = 0\n",
        "\n",
        "    return weights, bias\n",
        "\n",
        "def compute_cost(y, y_pred):\n",
        "    \"\"\"\n",
        "    Compute the cost function for logistic regression.\n",
        "\n",
        "    Parameters:\n",
        "    y : numpy array\n",
        "        Actual labels.\n",
        "    y_pred : numpy array\n",
        "        Predicted probabilities.\n",
        "\n",
        "    Returns:\n",
        "    float\n",
        "        The cost value.\n",
        "    \"\"\"\n",
        "    # compute the cost\n",
        "    epsilon = 1e-15\n",
        "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
        "\n",
        "    m = y.shape[0]\n",
        "    cost = -np.sum(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred)) / m\n",
        "\n",
        "    return cost\n",
        "\n",
        "def compute_gradients(X, y, y_pred):\n",
        "    \"\"\"\n",
        "    Compute the gradients for weights and bias.\n",
        "\n",
        "    Parameters:\n",
        "    X : numpy array\n",
        "        Feature matrix.\n",
        "    y : numpy array\n",
        "        Actual labels.\n",
        "    y_pred : numpy array\n",
        "        Predicted probabilities.\n",
        "\n",
        "    Returns:\n",
        "    tuple\n",
        "        Gradients of weights and bias.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    m = X.shape[0]\n",
        "\n",
        "    # compute dw\n",
        "    dw = np.dot(X.T, (y_pred - y)) / m\n",
        "\n",
        "    # compute db\n",
        "    db = np.sum(y_pred - y) / m\n",
        "\n",
        "    return dw, db\n",
        "\n",
        "\n",
        "def optimize(X, y, weights, bias, learning_rate, num_iterations):\n",
        "    \"\"\"\n",
        "    Perform gradient descent to optimize weights and bias.\n",
        "\n",
        "    Parameters:\n",
        "    X : numpy array\n",
        "        Feature matrix.\n",
        "    y : numpy array\n",
        "        Actual labels.\n",
        "    weights : numpy array\n",
        "        Weights of the model.\n",
        "    bias : float\n",
        "        Bias of the model.\n",
        "    learning_rate : float\n",
        "        Learning rate for gradient descent.\n",
        "    num_iterations : int\n",
        "        Number of iterations for gradient descent.\n",
        "\n",
        "    Returns:\n",
        "    tuple\n",
        "        Optimized weights, bias, and the list of costs.\n",
        "    \"\"\"\n",
        "    costs = []\n",
        "\n",
        "    for i in range(num_iterations):\n",
        "        # Compute linear model\n",
        "        z = np.dot(X, weights) + bias\n",
        "\n",
        "        # Apply sigmoid function\n",
        "        y_pred = sigmoid(z)\n",
        "\n",
        "        # Compute cost\n",
        "        cost = compute_cost(y,y_pred)\n",
        "        costs.append(cost)\n",
        "\n",
        "        # Compute gradients\n",
        "        dw ,db = compute_gradients(X,y,y_pred)\n",
        "\n",
        "        # Update weights and bias\n",
        "        weights -= learning_rate * dw\n",
        "        bias -= learning_rate * db\n",
        "\n",
        "    return weights, bias, costs\n",
        "\n",
        "def predict(X, weights, bias):\n",
        "    \"\"\"\n",
        "    Predict the binary labels for a dataset.\n",
        "\n",
        "    Parameters:\n",
        "    X : numpy array\n",
        "        Feature matrix.\n",
        "    weights : numpy array\n",
        "        Weights of the model.\n",
        "    bias : float\n",
        "        Bias of the model.\n",
        "\n",
        "    Returns:\n",
        "    numpy array\n",
        "        Predicted binary labels (0 or 1).\n",
        "    \"\"\"\n",
        "    z = np.dot(X, weights) + bias\n",
        "    y_pred = sigmoid(z)\n",
        "    predictions = [1 if i > 0.5 else 0 for i in y_pred]\n",
        "    return np.array(predictions)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# COMPUTE ACCURACY"
      ],
      "metadata": {
        "id": "v9rwH83Rwrfk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weights, bias = initialize_weights(X.shape[1])"
      ],
      "metadata": {
        "id": "-JihlJTp5naB"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(70,80):\n",
        "  weights, bias,costs = optimize(X,y,weights, bias,i/1000,65)\n",
        "  y_pred = predict(X_test,weights,bias)\n",
        "  matches = np.sum(y_test == y_pred)\n",
        "  mismatches = np.sum(y_test != y_pred)\n",
        "  print(f\"Accuracy: {matches/(matches+mismatches)},for learning rate {i/1000}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6meXoATXEu23",
        "outputId": "20f3288e-c625-40ae-8bbb-3d6531c20283"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.7082175885228031,for learning rate 0.07\n",
            "Accuracy: 0.7122613312704384,for learning rate 0.071\n",
            "Accuracy: 0.7167973557438728,for learning rate 0.072\n",
            "Accuracy: 0.712999753859137,for learning rate 0.073\n",
            "Accuracy: 0.7117338865642252,for learning rate 0.074\n",
            "Accuracy: 0.7146875769190196,for learning rate 0.075\n",
            "Accuracy: 0.7173951264109146,for learning rate 0.076\n",
            "Accuracy: 0.7148282288406765,for learning rate 0.077\n",
            "Accuracy: 0.7196103941770104,for learning rate 0.078\n",
            "Accuracy: 0.7104680192693132,for learning rate 0.079\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for j in range(65,71):\n",
        "   for i in range(71,81):\n",
        "      weights, bias,costs = optimize(X,y,weights, bias,i/1000,j)\n",
        "      y_pred = predict(X_test,weights,bias)\n",
        "      matches = np.sum(y_test == y_pred)\n",
        "      mismatches = np.sum(y_test != y_pred)\n",
        "      print(f\"Accuracy: {matches/(matches+mismatches)},for learning rate {i/1000},iterations{j}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KGmoYUwnGBFb",
        "outputId": "b5d542ee-41b2-4bf7-a440-46e38a2b0818"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.7153205105664756,for learning rate 0.071,iterations65\n",
            "Accuracy: 0.7144062730757059,for learning rate 0.072,iterations65\n",
            "Accuracy: 0.7180280600583705,for learning rate 0.073,iterations65\n",
            "Accuracy: 0.7136678504870073,for learning rate 0.074,iterations65\n",
            "Accuracy: 0.7221772917472485,for learning rate 0.075,iterations65\n",
            "Accuracy: 0.7176061042933999,for learning rate 0.076,iterations65\n",
            "Accuracy: 0.7178874081367137,for learning rate 0.077,iterations65\n",
            "Accuracy: 0.7161292591160027,for learning rate 0.078,iterations65\n",
            "Accuracy: 0.7200323499419811,for learning rate 0.079,iterations65\n",
            "Accuracy: 0.719540068216182,for learning rate 0.08,iterations65\n",
            "Accuracy: 0.7182038749604417,for learning rate 0.071,iterations66\n",
            "Accuracy: 0.720805935511094,for learning rate 0.072,iterations66\n",
            "Accuracy: 0.7225289215513907,for learning rate 0.073,iterations66\n",
            "Accuracy: 0.7168676817047013,for learning rate 0.074,iterations66\n",
            "Accuracy: 0.7194345792749394,for learning rate 0.075,iterations66\n",
            "Accuracy: 0.7190126235099688,for learning rate 0.076,iterations66\n",
            "Accuracy: 0.7215443580997926,for learning rate 0.077,iterations66\n",
            "Accuracy: 0.7186609937058265,for learning rate 0.078,iterations66\n",
            "Accuracy: 0.7179225711171279,for learning rate 0.079,iterations66\n",
            "Accuracy: 0.7214740321389641,for learning rate 0.08,iterations66\n",
            "Accuracy: 0.7216850100214495,for learning rate 0.071,iterations67\n",
            "Accuracy: 0.7216850100214495,for learning rate 0.072,iterations67\n",
            "Accuracy: 0.720981750413165,for learning rate 0.073,iterations67\n",
            "Accuracy: 0.7232673441400893,for learning rate 0.074,iterations67\n",
            "Accuracy: 0.7210169133935792,for learning rate 0.075,iterations67\n",
            "Accuracy: 0.7217553359822778,for learning rate 0.076,iterations67\n",
            "Accuracy: 0.7207004465698512,for learning rate 0.077,iterations67\n",
            "Accuracy: 0.7210872393544077,for learning rate 0.078,iterations67\n",
            "Accuracy: 0.7213685431977215,for learning rate 0.079,iterations67\n",
            "Accuracy: 0.7242167446112733,for learning rate 0.08,iterations67\n",
            "Accuracy: 0.7243573965329301,for learning rate 0.071,iterations68\n",
            "Accuracy: 0.7245332114350013,for learning rate 0.072,iterations68\n",
            "Accuracy: 0.7246387003762439,for learning rate 0.073,iterations68\n",
            "Accuracy: 0.7247793522979008,for learning rate 0.074,iterations68\n",
            "Accuracy: 0.7249551671999719,for learning rate 0.075,iterations68\n",
            "Accuracy: 0.7249200042195576,for learning rate 0.076,iterations68\n",
            "Accuracy: 0.7250606561412145,for learning rate 0.077,iterations68\n",
            "Accuracy: 0.7250606561412145,for learning rate 0.078,iterations68\n",
            "Accuracy: 0.7250254931608003,for learning rate 0.079,iterations68\n",
            "Accuracy: 0.7252013080628714,for learning rate 0.08,iterations68\n",
            "Accuracy: 0.7235838109638173,for learning rate 0.071,iterations69\n",
            "Accuracy: 0.7239354407679595,for learning rate 0.072,iterations69\n",
            "Accuracy: 0.7241464186504448,for learning rate 0.073,iterations69\n",
            "Accuracy: 0.7242870705721017,for learning rate 0.074,iterations69\n",
            "Accuracy: 0.7244628854741728,for learning rate 0.075,iterations69\n",
            "Accuracy: 0.7247793522979008,for learning rate 0.076,iterations69\n",
            "Accuracy: 0.7254122859453568,for learning rate 0.077,iterations69\n",
            "Accuracy: 0.7269594570835824,for learning rate 0.078,iterations69\n",
            "Accuracy: 0.7256584268082563,for learning rate 0.079,iterations69\n",
            "Accuracy: 0.7278736945743521,for learning rate 0.08,iterations69\n",
            "Accuracy: 0.7272407609268962,for learning rate 0.071,iterations70\n",
            "Accuracy: 0.7273110868877246,for learning rate 0.072,iterations70\n",
            "Accuracy: 0.7275923907310383,for learning rate 0.073,iterations70\n",
            "Accuracy: 0.7276978796722811,for learning rate 0.074,iterations70\n",
            "Accuracy: 0.7294911916734063,for learning rate 0.075,iterations70\n",
            "Accuracy: 0.7287879320651218,for learning rate 0.076,iterations70\n",
            "Accuracy: 0.7305460810858329,for learning rate 0.077,iterations70\n",
            "Accuracy: 0.7307922219487324,for learning rate 0.078,iterations70\n",
            "Accuracy: 0.7319526003024016,for learning rate 0.079,iterations70\n",
            "Accuracy: 0.7311086887724604,for learning rate 0.08,iterations70\n"
          ]
        }
      ]
    }
  ]
}